{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/local/softwares/anaconda3/envs/psycnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/data/local/softwares/anaconda3/envs/psycnet/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "/data/local/softwares/anaconda3/envs/psycnet/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path as osp\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from nilearn import datasets\n",
    "from nilearn.input_data import NiftiMasker\n",
    "from nilearn import plotting\n",
    "from nilearn import image\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "\n",
    "import nibabel as nib\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a csv file (Pandas Dataframe) for further machine learning\n",
    "\n",
    "def create_ml_csv(output_root_dir='/data_local/deeplearning/ABIDE_ML_inputs',\n",
    "                  sbc_dir = 'sbc'):\n",
    "    \n",
    "    df_data_info = pd.read_csv(osp.join(output_root_dir, 'data_info.csv'))\n",
    "    \n",
    "    #outputs\n",
    "    sbc_file_list = [] # seed_based filenames\n",
    "        \n",
    "    \n",
    "    for sub_i in df_data_info['SUB_ID']:\n",
    "        sbc_file_list.append(osp.join(output_root_dir, f'00{sub_i}', sbc_dir, 'ATLAS/BPTF/CONFOUNDS/sbc_maps.npy'))\n",
    "    \n",
    "    \n",
    "    df_data_info['sbc_file'] = sbc_file_list\n",
    "    \n",
    "    return df_data_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bandpass(to_bandpass = False, tr=1.0, low_freq=0.01, high_freq=0.001):\n",
    "    if to_bandpass:\n",
    "        masker = NiftiMasker(smoothing_fwhm=6, detrend=True, standardize=True, \n",
    "                                   t_r=tr, low_pass=low_freq, high_pass=high_freq, \n",
    "                                   memory='nilearn_cache', memory_level=1, verbose=0)\n",
    "    else:\n",
    "        masker = NiftiMasker(smoothing_fwhm=6, detrend=True, standardize=True, \n",
    "                                   memory='nilearn_cache', memory_level=1, verbose=0)\n",
    "        \n",
    "    return masker\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sbc_maps(timecourses, brain_masker, func_filename, confound_filename):\n",
    "    \n",
    "    brain_time_series = brain_masker.fit_transform(func_filename,\n",
    "                                               confounds=[confound_filename])\n",
    "    \n",
    "    sbc_maps = []\n",
    "    for seed_time_series in timecourses:\n",
    "        seed_based_correlations = np.dot(brain_time_series.T, seed_time_series) / seed_time_series.shape[0]\n",
    "        seed_based_correlations_fisher_z = np.arctanh(seed_based_correlations)\n",
    "        sbc_maps.append(np.squeeze(brain_masker.inverse_transform(seed_based_correlations_fisher_z.T).get_data()))\n",
    "        \n",
    "    return np.stack(sbc_maps)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sbc_maps(atlas_names, bptf, confounds,\n",
    "                    output_root_dir='/data_local/deeplearning/ABIDE_ML_inputs',\n",
    "                    sbc_dir = 'sbc',\n",
    "                    to_bandpass=False):\n",
    "\n",
    "    \n",
    "    df_data_info = create_ml_csv()\n",
    "    \n",
    "    # Write the generic input and output csv files\n",
    "    #df_data_info.to_csv(osp.join(output_root_dir, 'data_info.csv'))\n",
    "    \n",
    "    \n",
    "    nsubjects = len(df_data_info)\n",
    "    \n",
    "    print_counter = 0\n",
    "    \n",
    "    brain_masker = bandpass(to_bandpass=to_bandpass)\n",
    "\n",
    "    for sub_i in df_data_info.index:\n",
    "\n",
    "        \n",
    "        if print_counter%100 == 0:\n",
    "            print(f'{sub_i}/{nsubjects}')\n",
    "            \n",
    "        nuisance = pd.read_csv(df_data_info['nuisance_file'].loc[sub_i], sep='\\t', header=None)\n",
    "        nuisance.to_csv('temp_nuisance.csv') # required for the next step in csv format\n",
    "\n",
    "        for atlas_name in atlas_names:\n",
    "                \n",
    "            if bptf:\n",
    "                \n",
    "                rsfilename = df_data_info['RSFMRI_bptf_file'].loc[sub_i]\n",
    "                \n",
    "                if confounds:\n",
    "                    atlas_bptf_conf_name = osp.join(atlas_name, 'bptf/nilearn_regress')\n",
    "                    \n",
    "                else:\n",
    "                    atlas_bptf_conf_name = osp.join(atlas_name, 'bptf/no_nilearn_regress')\n",
    "              \n",
    "            else:\n",
    "                \n",
    "                rsfilename = df_data_info['RSFMRI_nonbptf_file'].loc[sub_i]\n",
    "                \n",
    "                if confounds:\n",
    "                    atlas_bptf_conf_name = osp.join(atlas_name, 'no_bptf/nilearn_regress')\n",
    "                else:\n",
    "                    atlas_bptf_conf_name = osp.join(atlas_name, 'no_bptf/no_nilearn_regress')\n",
    "                    \n",
    "\n",
    "            id_subject = df_data_info['SUB_ID'].loc[sub_i]        \n",
    "            file_dir = osp.join(output_root_dir, f'00{id_subject}' , \n",
    "                              sbc_dir, atlas_bptf_conf_name)\n",
    "            \n",
    "            if not osp.exists(file_dir):\n",
    "                os.makedirs(file_dir)\n",
    "\n",
    "            # Read timeseries as csv file\n",
    "            tc_file = df_data_info['tc_file'].loc[sub_i].replace('ATLAS/BPTF/CONFOUNDS', atlas_bptf_conf_name)\n",
    "            timecourses = pd.read_csv(tc_file).values.transpose() # resulting in nrois X ntimes matrix\n",
    "            \n",
    "            # Get sbc data \n",
    "            sbc_data = get_sbc_maps(timecourses, brain_masker, rsfilename, 'temp_nuisance.csv')\n",
    "            \n",
    "            # Write seed based correlation maps\n",
    "            sbc_file = df_data_info['sbc_file'].loc[sub_i].replace('ATLAS/BPTF/CONFOUNDS', atlas_bptf_conf_name)\n",
    "            np.save(sbc_file, sbc_data)\n",
    "            \n",
    "            \n",
    "            \n",
    "        print_counter += 1\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    \n",
    "    #atlas_names=['JAMA_IC19', 'JAMA_IC52', 'JAMA_IC7', 'AAL', 'HO_cort_maxprob_thr25-2mm', 'schaefer_100', 'schaefer_400']\n",
    "    atlas_names=['schaefer_100']\n",
    "\n",
    "    output_root_dir='/data_local/deeplearning/ABIDE_ML_inputs'\n",
    "    sbc_dir = 'sbc'\n",
    "    \n",
    "    for confs in [False]:\n",
    "        create_sbc_maps(atlas_names, bptf=True, confounds=confs, \n",
    "                        output_root_dir=output_root_dir, sbc_dir=sbc_dir)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/2169\n",
      "100/2169\n",
      "200/2169\n",
      "300/2169\n",
      "400/2169\n",
      "500/2169\n",
      "600/2169\n",
      "700/2169\n",
      "800/2169\n",
      "900/2169\n",
      "1000/2169\n",
      "1100/2169\n",
      "1200/2169\n",
      "1300/2169\n",
      "1400/2169\n",
      "1500/2169\n",
      "1600/2169\n",
      "1700/2169\n",
      "1800/2169\n",
      "1900/2169\n",
      "2000/2169\n",
      "2100/2169\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data exploration\n",
    "\n",
    "df = create_ml_csv()\n",
    "# plot ntimes to check what to include\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15,10))\n",
    "df.hist(column='nTimes', bins=[50, 100, 125, 150, 200, 300, 500, 1000], ax=axes[0])\n",
    "df.hist(column='nTimes', cumulative=-1, bins=[50, 100, 125, 150, 200, 300, 1000], linewidth=5, histtype='step', ax=axes[1])\n",
    "#df_input_data['nTimes'].plot.kde()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will work with the first subject of the adhd data set.\n",
    "# adhd_dataset.func is a list of filenames. We select the 1st (0-based)\n",
    "# subject by indexing with [0]).\n",
    "\n",
    "adhd_dataset = datasets.fetch_adhd(n_subjects=1)\n",
    "func_filename = adhd_dataset.func[0]\n",
    "confound_filename = adhd_dataset.confounds[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcc_coords = [(0, -52, 18)]\n",
    "from nilearn import input_data\n",
    "\n",
    "seed_masker = input_data.NiftiSpheresMasker(\n",
    "    pcc_coords, radius=8,\n",
    "    detrend=True, standardize=True,\n",
    "    low_pass=0.1, high_pass=0.01, t_r=2.,\n",
    "    memory='nilearn_cache', memory_level=1, verbose=0)\n",
    "\n",
    "seed_time_series = seed_masker.fit_transform(func_filename,\n",
    "                                             confounds=[confound_filename])\n",
    "brain_masker = input_data.NiftiMasker(\n",
    "    smoothing_fwhm=6,\n",
    "    detrend=True, standardize=True,\n",
    "    low_pass=0.1, high_pass=0.01, t_r=2.,\n",
    "    memory='nilearn_cache', memory_level=1, verbose=0)\n",
    "\n",
    "plt.plot(seed_time_series)\n",
    "plt.title('Seed time series (Posterior cingulate cortex)')\n",
    "plt.xlabel('Scan number')\n",
    "plt.ylabel('Normalized signal')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_time_series = brain_masker.fit_transform(func_filename,\n",
    "                                               confounds=[confound_filename])\n",
    "\n",
    "plt.plot(brain_time_series[:, [10, 45, 100, 5000, 10000]])\n",
    "plt.title('Time series from 5 random voxels')\n",
    "plt.xlabel('Scan number')\n",
    "plt.ylabel('Normalized signal')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_based_correlations = np.dot(brain_time_series.T, seed_time_series) / \\\n",
    "                          seed_time_series.shape[0]\n",
    "seed_based_correlations_fisher_z = np.arctanh(seed_based_correlations)\n",
    "seed_based_correlation_img = brain_masker.inverse_transform(\n",
    "    seed_based_correlations.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_based_correlation_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_based_correlations_fisher_z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_based_correlations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[]\n",
    "x.append(np.squeeze(seed_based_correlation_img.get_data()))\n",
    "x.append(np.squeeze(seed_based_correlation_img.get_data()))\n",
    "x.append(np.squeeze(seed_based_correlation_img.get_data()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx=np.stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "22311 - 22252"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
